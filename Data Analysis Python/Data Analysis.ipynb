{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we copy and mutate dataframes, which causes warnings, however we are aware of this\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis we use data, which we exported from our otree server.  \n",
    "The data was collected from the 21.08.2020 to the 26.08.2020 12:00.  \n",
    " \n",
    "We start by loading that data into a dataframe from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data26aug12uhr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we rename the most important data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id']=df['participant.id_in_session']\n",
    "df['group']=df['bms_experiment.1.player.trans_cond']\n",
    "df['age']=df['bms_experiment.1.player.age']\n",
    "df['gender']=df['bms_experiment.1.player.gender']\n",
    "df['installed']=df['bms_experiment.1.player.installed']\n",
    "df['per_und_r1']=df['bms_experiment.1.player.understanding']\n",
    "df['per_und_r2']=df['bms_experiment.2.player.understanding']\n",
    "df['color_try1']=df['bms_experiment.1.player.attentive_1']\n",
    "df['color_try2']=df['bms_experiment.1.player.attentive_2']\n",
    "df['finished']=((df['bms_experiment.1.player.finished'] == 1.0) | (df['bms_experiment.2.player.finished'] == 1.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort out all participants who did not finish the whole survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['finished'] == True] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Understanding Calculation\n",
    "Here we calculate the actual understanding, which is determined by four multiple-choice-questions.  \n",
    "The participants will get one point per possible answer, if they answered according to the solution. This leads to four possible points per question with four questions in total.  \n",
    "There also was the possibility to answer with \"I don't know\". If this is the case the participant will automatically get zero points for the question, regardless the other answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize columns for each question\n",
    "\n",
    "# round 1\n",
    "df['q1_r1']= 0\n",
    "df['q2_r1']= 0\n",
    "df['q3_r1']= 0\n",
    "df['q4_r1']= 0\n",
    "\n",
    "#round 2\n",
    "df['q1_r2']= 0\n",
    "df['q2_r2']= 0\n",
    "df['q3_r2']= 0\n",
    "df['q4_r2']= 0\n",
    "\n",
    "# 1 => answer is correct\n",
    "# 0 => answer is wrong\n",
    "solutions = [[0,0,1,0],[0,0,0,1],[1,0,0,1],[0,0,0,1]]\n",
    "\n",
    "# we iterate through all rounds (1 & 2) and all questions (1 - 4)\n",
    "for roundnumber in range(1,3):\n",
    "    for question in range(1,5):\n",
    "        for i in df.T:\n",
    "            question_score = 0\n",
    "            # if the participants answered \"I don't know, we set the question_score to zero\"\n",
    "            if df['bms_experiment.{0}.player.q{1}_a5'.format(roundnumber,question)][i] == 1:\n",
    "                question_score = 0\n",
    "            else:\n",
    "                # we iterate through all possible answers and increase the question score by one for each correct question\n",
    "                for answer in range(1,5):\n",
    "                    if df['bms_experiment.{0}.player.q{1}_a{2}'.format(roundnumber,question,answer)][i] == solutions[question-1][answer-1]:\n",
    "                        question_score = question_score + 1 \n",
    "                # after calculating the question score we set it accordingly in the dataframe \n",
    "                df['q{0}_r{1}'.format(question,roundnumber)][i]=question_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency in inverted questions\n",
    "All of our trust questions also have an inverted form. We did this to be aware of people, who simply answer the survey randomly.  \n",
    "Here we set a column called \"invertedCheck\" to False if the participant answered strongly contradictory.  \n",
    "By strongly contradictory we mean that the participant answered \"Fully agree\" to both the original as well as the negated question. \n",
    "Technically this means that the participant answered 5 to both questions. We calculate the invertedCheck by calculating the average of both answers. If this average is equal to 1 or 5, this means that the participant answered strongly contradictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize column for invertedCheck\n",
    "df['invertedCheck'] = True\n",
    "\n",
    "# iterate through each round and each trust aspect\n",
    "for roundnumber in range(1,3):\n",
    "    for trustAspect in ['competence','benevolence','no_central_entity','anonymity','no_tracking','unlinkabilty']:\n",
    "        for i in df.T:\n",
    "            if (((df['bms_experiment.{0}.player.{1}'.format(roundnumber, trustAspect)][i] +\n",
    "                  df['bms_experiment.{0}.player.{1}_neg'.format(roundnumber, trustAspect)][i]) / 2) < 2) | \\\n",
    "                    (((df['bms_experiment.{0}.player.{1}'.format(roundnumber, trustAspect)][i] +\n",
    "                       df['bms_experiment.{0}.player.{1}_neg'.format(roundnumber, trustAspect)][i]) / 2) > 4):\n",
    "                df['invertedCheck'][i]= False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate trust\n",
    "Here we calculate trust values for competence and benevolence, as well as the subaspects of integrity.  \n",
    "For each value we have exactly one question and one negated question. We calculate the corresponding value through the average of the answer to the original question and the negated question. Before doing that we invert the answer to the negated question. This means that if the answer to the negated question is 5, we change it 1. (5->1, 4->2, 3->3, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each round and each trust aspect\n",
    "for roundnumber in range(1,3):\n",
    "    for trustAspect in ['competence','benevolence','no_central_entity','anonymity','no_tracking','unlinkabilty']:\n",
    "        df['{0}_r{1}'.format(trustAspect,roundnumber)]= 100 #column erstellen\n",
    "        for i in df.T:\n",
    "            aspect = df['bms_experiment.{0}.player.{1}'.format(roundnumber,trustAspect)][i]\n",
    "            # invert the negated question\n",
    "            aspect_neg_inverted = abs(df['bms_experiment.{0}.player.{1}_neg'.format(roundnumber,trustAspect)][i]-6)\n",
    "            # calculate question and save to dataframe\n",
    "            df['{0}_r{1}'.format(trustAspect,roundnumber)][i] = (aspect + aspect_neg_inverted)/2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['participant.id_in_session', 'bms_experiment.1.player.trans_cond',\n",
    "       'bms_experiment.1.player.age', 'bms_experiment.1.player.gender',\n",
    "       'bms_experiment.1.player.installed',\n",
    "       'bms_experiment.1.player.understanding',\n",
    "       'bms_experiment.1.player.attentive_1',\n",
    "       'bms_experiment.1.player.attentive_2',\n",
    "       'bms_experiment.1.player.finished',\n",
    "       'bms_experiment.2.player.understanding',\n",
    "       'bms_experiment.2.player.finished',\n",
    "       'participant._is_bot','participant.code','participant.label','participant._index_in_pages',\n",
    "       'participant._max_page_index', 'participant._current_app_name',\n",
    "       'participant._current_page_name', 'participant.time_started',\n",
    "       'participant.visited','participant.mturk_worker_id',\n",
    "       'participant.mturk_assignment_id', 'participant.payoff',\n",
    "       'participant.payoff_plus_participation_fee', 'session.code',\n",
    "       'session.label', 'session.mturk_HITId', 'session.mturk_HITGroupId',\n",
    "       'session.comment', 'session.is_demo',\n",
    "       'session.config.participation_fee',\n",
    "       'session.config.real_world_currency_per_point','bms_experiment.1.player.payoff','bms_experiment.2.player.payoff','bms_experiment.1.player.id_in_group',\n",
    "       'bms_experiment.1.group.id_in_subsession',\n",
    "       'bms_experiment.1.subsession.round_number',\n",
    "       'bms_experiment.2.player.id_in_group',\n",
    "       'bms_experiment.2.player.trans_cond','bms_experiment.2.group.id_in_subsession',\n",
    "       'bms_experiment.2.subsession.round_number','bms_experiment.2.player.age',\n",
    "       'bms_experiment.2.player.gender', 'bms_experiment.2.player.installed',\n",
    "       'bms_experiment.2.player.attentive_1',\n",
    "       'bms_experiment.2.player.attentive_2',\n",
    "       'bms_experiment.1.player.q1_a1', 'bms_experiment.1.player.q1_a2',\n",
    "       'bms_experiment.1.player.q1_a3', 'bms_experiment.1.player.q1_a4',\n",
    "       'bms_experiment.1.player.q1_a5', 'bms_experiment.1.player.q2_a1',\n",
    "       'bms_experiment.1.player.q2_a2', 'bms_experiment.1.player.q2_a3',\n",
    "       'bms_experiment.1.player.q2_a4', 'bms_experiment.1.player.q2_a5',\n",
    "       'bms_experiment.1.player.q3_a1', 'bms_experiment.1.player.q3_a2',\n",
    "       'bms_experiment.1.player.q3_a3', 'bms_experiment.1.player.q3_a4',\n",
    "       'bms_experiment.1.player.q3_a5', 'bms_experiment.1.player.q4_a1',\n",
    "       'bms_experiment.1.player.q4_a2', 'bms_experiment.1.player.q4_a3',\n",
    "       'bms_experiment.1.player.q4_a4', 'bms_experiment.1.player.q4_a5',\n",
    "       'bms_experiment.2.player.q1_a1', 'bms_experiment.2.player.q1_a2',\n",
    "       'bms_experiment.2.player.q1_a3', 'bms_experiment.2.player.q1_a4',\n",
    "       'bms_experiment.2.player.q1_a5', 'bms_experiment.2.player.q2_a1',\n",
    "       'bms_experiment.2.player.q2_a2', 'bms_experiment.2.player.q2_a3',\n",
    "       'bms_experiment.2.player.q2_a4', 'bms_experiment.2.player.q2_a5',\n",
    "       'bms_experiment.2.player.q3_a1', 'bms_experiment.2.player.q3_a2',\n",
    "       'bms_experiment.2.player.q3_a3', 'bms_experiment.2.player.q3_a4',\n",
    "       'bms_experiment.2.player.q3_a5', 'bms_experiment.2.player.q4_a1',\n",
    "       'bms_experiment.2.player.q4_a2', 'bms_experiment.2.player.q4_a3',\n",
    "       'bms_experiment.2.player.q4_a4', 'bms_experiment.2.player.q4_a5',\n",
    "       'bms_experiment.1.player.competence',\n",
    "       'bms_experiment.1.player.competence_neg',\n",
    "       'bms_experiment.1.player.benevolence',\n",
    "       'bms_experiment.1.player.benevolence_neg',\n",
    "       'bms_experiment.1.player.no_central_entity',\n",
    "       'bms_experiment.1.player.no_central_entity_neg',\n",
    "       'bms_experiment.1.player.anonymity',\n",
    "       'bms_experiment.1.player.anonymity_neg',\n",
    "       'bms_experiment.1.player.no_tracking',\n",
    "       'bms_experiment.1.player.no_tracking_neg',\n",
    "       'bms_experiment.1.player.unlinkabilty',\n",
    "       'bms_experiment.1.player.unlinkabilty_neg',\n",
    "        'bms_experiment.2.player.competence',\n",
    "       'bms_experiment.2.player.competence_neg',\n",
    "       'bms_experiment.2.player.benevolence',\n",
    "       'bms_experiment.2.player.benevolence_neg',\n",
    "       'bms_experiment.2.player.no_central_entity',\n",
    "       'bms_experiment.2.player.no_central_entity_neg',\n",
    "       'bms_experiment.2.player.anonymity',\n",
    "       'bms_experiment.2.player.anonymity_neg',\n",
    "       'bms_experiment.2.player.no_tracking',\n",
    "       'bms_experiment.2.player.no_tracking_neg',\n",
    "       'bms_experiment.2.player.unlinkabilty',\n",
    "       'bms_experiment.2.player.unlinkabilty_neg',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color check\n",
    "In our survey we added a simple piece of information in the text. It said \"The color is yellow\".  \n",
    "After the participants read that text, they had to replicate this information. If they failed the first time, they were asked to read the text again and had another chance to replicate. If they failed a second time we will sort them out.  \n",
    "The control group had no text and thereby they never failed this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['color_check'] = (df['group'] == 'no') | ( # set to true if group is control group\n",
    "            (df['group'] != 'no') & ((df['color_try1'] == 'yellow') | (df['color_try2'] == 'yellow'))) # set to true if participant answered correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of actual understanding and integrity\n",
    "We calculate the actual understanding (in both rounds) through the average the points for each question.  \n",
    "Integrity will also be calculated through the average of each subaspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['act_und_r1']=(df_new['q1_r1']+df_new['q2_r1']+df_new['q3_r1']+df_new['q4_r1'])/4\n",
    "df_new['act_und_r2']=(df_new['q1_r2']+df_new['q2_r2']+df_new['q3_r2']+df_new['q4_r2'])/4\n",
    "df_new['integrity_r1']=(df_new['no_central_entity_r1']+df_new['anonymity_r1']+df_new['no_tracking_r1']+df_new['unlinkabilty_r1'])/4\n",
    "df_new['integrity_r2']=(df_new['no_central_entity_r2']+df_new['anonymity_r2']+df_new['no_tracking_r2']+df_new['unlinkabilty_r2'])/4\n",
    "\n",
    "# drop the columns which were used for calculation\n",
    "df_new.drop(['q1_r1','q2_r1', 'q3_r1', 'q4_r1', 'q1_r2', 'q2_r2', 'q3_r2', 'q4_r2',\n",
    "        'no_central_entity_r1', 'anonymity_r1', 'no_tracking_r1','unlinkabilty_r1',\n",
    "        'no_central_entity_r2', 'anonymity_r2', 'no_tracking_r2',\n",
    "       'unlinkabilty_r2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of trust\n",
    "We calculate trust through the average of competence, benevolence and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we create new columns in which we can store trust aspects for both groups\n",
    "df_new['competence']=0\n",
    "df_new['benevolence']=0\n",
    "df_new['integrity']=0\n",
    "\n",
    "for i in df_new.T:\n",
    "    if df_new['group'][i] == 'no':\n",
    "        df_new['competence'][i]=df_new['competence_r1'][i]\n",
    "        df_new['benevolence'][i]=df_new['benevolence_r1'][i]\n",
    "        df_new['integrity'][i]=df_new['integrity_r1'][i]\n",
    "    else:\n",
    "        df_new['competence'][i]=df_new['competence_r2'][i]\n",
    "        df_new['benevolence'][i]=df_new['benevolence_r2'][i]\n",
    "        df_new['integrity'][i]=df_new['integrity_r2'][i]\n",
    "        \n",
    "\n",
    "df_new['trust']=(df_new['competence']+df_new['benevolence']+df_new['integrity'])/3\n",
    "\n",
    "# drop the columns used for calculation\n",
    "df_new.drop(['competence_r1', 'benevolence_r1', 'competence_r2', 'benevolence_r2','integrity_r1', 'integrity_r2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of participants: 77\n",
      "      failed the color check: 7\n",
      "    \n",
      "number of valid participants: 70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = df_new['id'].count()\n",
    "failed_color = df_new[df_new['color_check']==False]['id'].count()\n",
    "valid = total-failed_color\n",
    "print(\"\"\"total number of participants: {}\n",
    "      failed the color check: {}\n",
    "    \n",
    "number of valid participants: {}\n",
    "\"\"\".format(total, failed_color, valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_new[(df_new['finished']==True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "First we take a look at some basic information about our participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average age:\n",
      "~25.61\n",
      "\n",
      "percentage of participants, who have the app installed:\n",
      "80.52%\n",
      "\n",
      "female:  41.56%\n",
      "male:    58.44%\n",
      "diverse: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_female = df_new[df_new['gender'] == 0]['id'].count()\n",
    "num_male = df_new[df_new['gender'] == 1]['id'].count()\n",
    "num_diverse = df_new[df_new['gender'] == 2]['id'].count()\n",
    "\n",
    "print(\"\"\"average age:\n",
    "~{:.2f}\n",
    "\n",
    "percentage of participants, who have the app installed:\n",
    "{:.2f}%\n",
    "\n",
    "female:  {:.2f}%\n",
    "male:    {:.2f}%\n",
    "diverse: {:.2f}%\n",
    "\"\"\".format(df_new['age'].mean(),\n",
    "           df_new['installed'].mean()*100,\n",
    "           num_female/total*100,\n",
    "           num_male/total*100,\n",
    "           num_diverse/total*100\n",
    "          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [H1] \"Users will have a better understanding of the system when provided with an explanation of its   functionality.\"\n",
    "#### Control vs Treatment\n",
    "##### Actual Understanding\n",
    "We retrieve the actual understanding of the control group and the actual understanding of the treatment groups after the treatment. After that we will check if there is a statistically significant difference between the control group and the first treatment group as well as the control group and the second treatment group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr dataset:  27\n",
      "tr dataset:  25\n",
      "tr dataset:  25\n",
      "Number of participants in the...\n",
      "... control group:    27\n",
      "... 'brief' group:    25\n",
      "... 'detailed' group: 25\n",
      "\n",
      "Mean actual understanding of participants in the...\n",
      "... control group:    ~2.9815\n",
      "... 'brief' group:    ~3.2100\n",
      "... 'detailed' group: ~3.2800\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr1=df_clean.loc[df_clean['group']=='no']['act_und_r1']\n",
    "print('tr dataset: ', len(tr1))\n",
    "tr2=df_clean.loc[df_clean['group']=='brief']['act_und_r2']\n",
    "print('tr dataset: ', len(tr2))\n",
    "tr3=df_clean.loc[df_clean['group']=='detailed']['act_und_r2']\n",
    "print('tr dataset: ', len(tr3))\n",
    "\n",
    "print(\"\"\"Number of participants in the...\n",
    "... control group:    {}\n",
    "... 'brief' group:    {}\n",
    "... 'detailed' group: {}\n",
    "\n",
    "Mean actual understanding of participants in the...\n",
    "... control group:    ~{:.4f}\n",
    "... 'brief' group:    ~{:.4f}\n",
    "... 'detailed' group: ~{:.4f}\n",
    "\n",
    "\"\"\".format(len(tr1), len(tr2), len(tr2),\n",
    "          np.mean(tr1), np.mean(tr2), np.mean(tr3)\n",
    "          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run a MWU-Test, because it is non-parametic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control group vs. 'brief' group \n",
      " MannwhitneyuResult(statistic=282.5, pvalue=0.3138694855553543)\n",
      "\n",
      "control group vs. 'detailed' group \n",
      " MannwhitneyuResult(statistic=266.0, pvalue=0.18772932289377164)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(tr1,tr2,alternative='two-sided')\n",
    "print(\"control group vs. 'brief' group \\n\", MWU)\n",
    "print(\"\")\n",
    "MWU=stats.mannwhitneyu(tr1,tr3,alternative='two-sided')\n",
    "print(\"control group vs. 'detailed' group \\n\", MWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perceived Understanding\n",
    "We repeat the process for perceived understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean perceived understanding of participants in the...\n",
      "... control group:    ~3.5556\n",
      "... 'brief' group:    ~3.7200\n",
      "... 'detailed' group: ~4.0000\n",
      "\n",
      "\n",
      "control group vs. 'brief' group \n",
      " MannwhitneyuResult(statistic=282.5, pvalue=0.3138694855553543)\n",
      "\n",
      "control group vs. 'detailed' group \n",
      " MannwhitneyuResult(statistic=266.0, pvalue=0.18772932289377164)\n"
     ]
    }
   ],
   "source": [
    "ptr1=df_clean.loc[df_clean['group']=='no']['per_und_r1']\n",
    "ptr2=df_clean.loc[df_clean['group']=='brief']['per_und_r2']\n",
    "ptr3=df_clean.loc[df_clean['group']=='detailed']['per_und_r2']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean perceived understanding of participants in the...\n",
    "... control group:    ~{:.4f}\n",
    "... 'brief' group:    ~{:.4f}\n",
    "... 'detailed' group: ~{:.4f}\n",
    "\n",
    "\"\"\".format(np.mean(ptr1), np.mean(ptr2), np.mean(ptr3)))\n",
    "\n",
    "MWU=stats.mannwhitneyu(tr1,tr2,alternative='two-sided')\n",
    "print(\"control group vs. 'brief' group \\n\", MWU)\n",
    "print(\"\")\n",
    "MWU=stats.mannwhitneyu(tr1,tr3,alternative='two-sided')\n",
    "print(\"control group vs. 'detailed' group \\n\", MWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treatment before vs after\n",
    "We can also compare the trust inside the treatment groups, because we collected data on understanding before the texts and after the texts.\n",
    "##### Actual Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean actual understanding of participants...\n",
      "... before the treatment:    ~2.8800\n",
      "... after the treatment:     ~3.2450\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "before=df_clean.loc[(df_clean['group']=='brief') | (df_clean['group']=='detailed')]['act_und_r1']\n",
    "after=df_clean.loc[(df_clean['group']=='brief') | (df_clean['group']=='detailed')]['act_und_r2']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean actual understanding of participants...\n",
    "... before the treatment:    ~{:.4f}\n",
    "... after the treatment:     ~{:.4f}\n",
    "\n",
    "\"\"\".format(np.mean(before), np.mean(after)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before treatment vs. after treatment \n",
      " MannwhitneyuResult(statistic=947.0, pvalue=0.03507584276644766)\n"
     ]
    }
   ],
   "source": [
    "#comparing actual understanding before and after reading the text\n",
    "MWU=stats.mannwhitneyu(before,after,alternative='two-sided') #MWU Test, non-parametic for independent samples\n",
    "print(\"before treatment vs. after treatment \\n\", MWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perceived Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean actual understanding of participants...\n",
      "... before the treatment:    ~3.5800\n",
      "... after the treatment:     ~3.8600\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "before=df_clean.loc[(df_clean['group']=='brief') | (df_clean['group']=='detailed')]['per_und_r1']\n",
    "after=df_clean.loc[(df_clean['group']=='brief') | (df_clean['group']=='detailed')]['per_und_r2']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean actual understanding of participants...\n",
    "... before the treatment:    ~{:.4f}\n",
    "... after the treatment:     ~{:.4f}\n",
    "\n",
    "\"\"\".format(np.mean(before), np.mean(after)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before treatment vs. after treatment \n",
      " MannwhitneyuResult(statistic=1084.0, pvalue=0.23114250742540687)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(before,after,alternative='two-sided') #MWU Test, non-parametic for independent samples\n",
    "print(\"before treatment vs. after treatment \\n\", MWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [H2] Understandable explanations of the appâ€™s functionality will increase trust more than other types of transparency features.\n",
    "We will compare the trust of the control group with the trust of the treatment groups. Our hypothesis states that the trust should be significantly higher in the treatment groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treatment = df_clean.loc[(df_clean['group']=='brief') | (df_clean['group']=='detailed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2c432865148>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x='act_und_r2', y='trust', data=df_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean trust of participants...\n",
      "... in the control group:       ~2.8025\n",
      "... im the treatment group:     ~2.8467\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_trust = df_treatment['trust']\n",
    "ctrl_trust = df_clean.loc[(df_clean['group']=='no')]['trust']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean trust of participants...\n",
    "... in the control group:       ~{:.4f}\n",
    "... im the treatment group:     ~{:.4f}\n",
    "\n",
    "\"\"\".format(np.mean(ctrl_trust), np.mean(tr_trust)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trust in control group vs. trust in treatment groups \n",
      " MannwhitneyuResult(statistic=736.5, pvalue=0.49431564722359067)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(tr_trust,ctrl_trust,alternative='two-sided') #MWU Test, non-parametic for independent samples\n",
    "print(\"trust in control group vs. trust in treatment groups \\n\", MWU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean trust in competence of participants...\n",
      "... in the control group:       ~3.6296\n",
      "... im the treatment group:     ~4.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_competence = df_treatment['competence']\n",
    "ctrl_competence = df_clean.loc[(df_clean['group']=='no')]['competence']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean trust in competence of participants...\n",
    "... in the control group:       ~{:.4f}\n",
    "... im the treatment group:     ~{:.4f}\n",
    "\n",
    "\"\"\".format(np.mean(ctrl_competence), np.mean(tr_competence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trust in competence in control group vs. trust in competence in treatment groups \n",
      " MannwhitneyuResult(statistic=839.0, pvalue=0.06369525134362775)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(tr_competence,ctrl_competence,alternative='two-sided')\n",
    "print(\"trust in competence in control group vs. trust in competence in treatment groups \\n\", MWU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean trust in benevolence of participants...\n",
      "... in the control group:       ~1.5185\n",
      "... im the treatment group:     ~1.2400\n",
      "\n",
      "Belief in benevolence actually decreases!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_benevolence = df_treatment['benevolence']\n",
    "ctrl_benevolence = df_clean.loc[(df_clean['group']=='no')]['benevolence']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean trust in benevolence of participants...\n",
    "... in the control group:       ~{:.4f}\n",
    "... im the treatment group:     ~{:.4f}\n",
    "\n",
    "Belief in benevolence actually decreases!\n",
    "\n",
    "\"\"\".format(np.mean(ctrl_benevolence), np.mean(tr_benevolence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trust in benevolence in control group vs. trust in benevolence in treatment groups \n",
      " MannwhitneyuResult(statistic=554.0, pvalue=0.09313253666177845)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(tr_benevolence,ctrl_benevolence,alternative='two-sided') #MWU Test, non-parametic for independent samples\n",
    "print(\"trust in benevolence in control group vs. trust in benevolence in treatment groups \\n\", MWU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean trust in integrity of participants...\n",
      "... in the control group:       ~3.2593\n",
      "... im the treatment group:     ~3.3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_integrity = df_treatment['integrity']\n",
    "ctrl_integrity = df_clean.loc[(df_clean['group']=='no')]['integrity']\n",
    "\n",
    "print(\"\"\"\n",
    "Mean trust in integrity of participants...\n",
    "... in the control group:       ~{:.4f}\n",
    "... im the treatment group:     ~{:.4f}\n",
    "\n",
    "\"\"\".format(np.mean(ctrl_integrity), np.mean(tr_integrity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trust in integrity in control group vs. trust in integrity in treatment groups \n",
      " MannwhitneyuResult(statistic=709.5, pvalue=0.6596535664455245)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(tr_integrity,ctrl_integrity,alternative='two-sided') #MWU Test, non-parametic for independent samples\n",
    "print(\"trust in integrity in control group vs. trust in integrity in treatment groups \\n\", MWU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "probably depricated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  trust   R-squared:                       0.056\n",
      "Model:                            OLS   Adj. R-squared:                  0.037\n",
      "Method:                 Least Squares   F-statistic:                     2.870\n",
      "Date:                Wed, 26 Aug 2020   Prob (F-statistic):             0.0967\n",
      "Time:                        20:05:50   Log-Likelihood:                -11.606\n",
      "No. Observations:                  50   AIC:                             27.21\n",
      "Df Residuals:                      48   BIC:                             31.04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.5029      0.208     12.054      0.000       2.085       2.920\n",
      "act_und_r2     0.1059      0.063      1.694      0.097      -0.020       0.232\n",
      "==============================================================================\n",
      "Omnibus:                        0.584   Durbin-Watson:                   1.771\n",
      "Prob(Omnibus):                  0.747   Jarque-Bera (JB):                0.401\n",
      "Skew:                          -0.217   Prob(JB):                        0.818\n",
      "Kurtosis:                       2.940   Cond. No.                         17.0\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "reg=smf.ols('trust~act_und_r2',data=df_treatment).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             competence   R-squared:                       0.086\n",
      "Model:                            OLS   Adj. R-squared:                  0.067\n",
      "Method:                 Least Squares   F-statistic:                     4.496\n",
      "Date:                Wed, 26 Aug 2020   Prob (F-statistic):             0.0392\n",
      "Time:                        20:05:50   Log-Likelihood:                -59.067\n",
      "No. Observations:                  50   AIC:                             122.1\n",
      "Df Residuals:                      48   BIC:                             126.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.8883      0.536      5.384      0.000       1.810       3.967\n",
      "act_und_r2     0.3426      0.162      2.120      0.039       0.018       0.667\n",
      "==============================================================================\n",
      "Omnibus:                        2.277   Durbin-Watson:                   1.788\n",
      "Prob(Omnibus):                  0.320   Jarque-Bera (JB):                2.190\n",
      "Skew:                          -0.464   Prob(JB):                        0.335\n",
      "Kurtosis:                       2.566   Cond. No.                         17.0\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            benevolence   R-squared:                       0.170\n",
      "Model:                            OLS   Adj. R-squared:                  0.153\n",
      "Method:                 Least Squares   F-statistic:                     9.834\n",
      "Date:                Wed, 26 Aug 2020   Prob (F-statistic):            0.00292\n",
      "Time:                        20:05:50   Log-Likelihood:                -32.840\n",
      "No. Observations:                  50   AIC:                             69.68\n",
      "Df Residuals:                      48   BIC:                             73.50\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.2131      0.318      6.970      0.000       1.575       2.851\n",
      "act_und_r2    -0.2999      0.096     -3.136      0.003      -0.492      -0.108\n",
      "==============================================================================\n",
      "Omnibus:                       21.592   Durbin-Watson:                   1.507\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               29.480\n",
      "Skew:                           1.551   Prob(JB):                     3.97e-07\n",
      "Kurtosis:                       5.129   Cond. No.                         17.0\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              integrity   R-squared:                       0.129\n",
      "Model:                            OLS   Adj. R-squared:                  0.111\n",
      "Method:                 Least Squares   F-statistic:                     7.139\n",
      "Date:                Wed, 26 Aug 2020   Prob (F-statistic):             0.0103\n",
      "Time:                        20:05:50   Log-Likelihood:                -36.534\n",
      "No. Observations:                  50   AIC:                             77.07\n",
      "Df Residuals:                      48   BIC:                             80.89\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.4074      0.342      7.042      0.000       1.720       3.095\n",
      "act_und_r2     0.2751      0.103      2.672      0.010       0.068       0.482\n",
      "==============================================================================\n",
      "Omnibus:                        6.097   Durbin-Watson:                   1.362\n",
      "Prob(Omnibus):                  0.047   Jarque-Bera (JB):                2.392\n",
      "Skew:                           0.158   Prob(JB):                        0.302\n",
      "Kurtosis:                       1.976   Cond. No.                         17.0\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#three components\n",
    "reg1=smf.ols('competence~act_und_r2',data=df_treatment).fit()\n",
    "print(reg1.summary())\n",
    "reg2=smf.ols('benevolence~act_und_r2',data=df_treatment).fit()\n",
    "print(reg2.summary())\n",
    "reg3=smf.ols('integrity~act_und_r2',data=df_treatment).fit()\n",
    "print(reg3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probably depricated\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [H3]: Reasoned explanations of the appâ€™s functionality will increase trust more than other types of transparency features.\n",
    "We will test whether trust in the treatment group with transparency condition 'detailed' increased more than the treatment group with transparency condition 'brief'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we do not use the control group for this hypothesis we will remove participants, which got the color check wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_valid = df_clean[df_clean['color_check'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants in the...\n",
      "... 'brief' group:    23\n",
      "... 'detailed' group: 20\n",
      "\n",
      "Mean trust of participants in the...\n",
      "... 'brief' group:    ~2.8116\n",
      "... 'detailed' group: ~2.9167\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_brief=df_clean_valid.loc[df_clean_valid['group']=='brief']['trust']\n",
    "tr_detailed=df_clean_valid.loc[df_clean_valid['group']=='detailed']['trust']\n",
    "\n",
    "print(\"\"\"Number of participants in the...\n",
    "... 'brief' group:    {}\n",
    "... 'detailed' group: {}\n",
    "\n",
    "Mean trust of participants in the...\n",
    "... 'brief' group:    ~{:.4f}\n",
    "... 'detailed' group: ~{:.4f}\n",
    "\n",
    "\"\"\".format(len(tr_brief), len(tr_detailed),\n",
    "          np.mean(tr_brief), np.mean(tr_detailed),\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trust in 'brief' group vs. trust in 'detailed' group \n",
      " MannwhitneyuResult(statistic=181.0, pvalue=0.21253369484977003)\n"
     ]
    }
   ],
   "source": [
    "MWU=stats.mannwhitneyu(tr_brief,tr_detailed,alternative='two-sided') #MWU Test, non-parametic for independent samples\n",
    "print(\"trust in 'brief' group vs. trust in 'detailed' group \\n\", MWU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
